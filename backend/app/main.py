"""
FastAPI Backend - × ×™×”×•×œ ×ª×œ×•×©×™ ×©×›×¨
"""
from fastapi import FastAPI, UploadFile, File, Depends, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from sqlalchemy.orm import Session
from typing import List, Optional, Dict
from pydantic import BaseModel
from typing import Optional
from datetime import datetime
import shutil
import os
from pathlib import Path
import sys

# Add backend directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.database import get_db, init_db, Payslip, FeedbackEntry, ChatHistory, AgentLearning, SavedKPI, KnowledgeInsight
from app.pdf_parser import HebrewPayslipPDFParser
from app.ai_agent.learning_manager import LearningManager

# Import from new structure
from crewai import Crew, Process
from config import claude_llm
from agents import chatbot_manager, parser, validator, analyzer, designer, reporter
from tasks import (
    parse_task, validate_task, analyze_task, report_task,
    chatbot_coordination_task
)
# from learning.knowledge_base import KnowledgeBase  # TODO: Move to backend structure

app = FastAPI(title="Payslip Analysis API", version="1.0.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:8080",
        "http://localhost:3000",
        "http://localhost:9000"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize
UPLOAD_DIR = Path("/app/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

pdf_parser = HebrewPayslipPDFParser()
# knowledge_base = KnowledgeBase()  # TODO: Move to backend structure
analysis_crew = None  # × ××ª×—×œ ×‘-startup

# Create Payslip Analysis Crew
payslip_crew = Crew(
    agents=[chatbot_manager, parser, validator, analyzer, reporter],
    tasks=[parse_task, validate_task, analyze_task, report_task],
    process=Process.hierarchical,
    manager_llm=claude_llm,
    verbose=True
)


@app.on_event("startup")
async def startup_event():
    """
    ××ª×—×•×œ ×‘×”×¤×¢×œ×”
    """
    global analysis_crew

    print("ğŸš€ Starting Payslip Analysis API...")

    # ×™×¦×™×¨×ª ×˜×‘×œ××•×ª
    init_db()
    print("âœ“ Database initialized")

    # Crew ×›×‘×¨ ××•×›×Ÿ ×›-global variable
    analysis_crew = payslip_crew
    print("âœ“ Payslip Analysis Crew ready (Hierarchical + Chat Bot Manager)")
    print("âœ“ Agents: Chat Bot Manager, Parser, Validator, Analyzer, Reporter")
    print("âœ“ Process: Hierarchical with Manager LLM")

    print("âœ“ Server ready on port 3000")


@app.get("/")
async def root():
    """
    ×‘×“×™×§×ª ×‘×¨×™××•×ª
    """
    return {
        "status": "running",
        "service": "Payslip Analysis API",
        "version": "1.0.0"
    }


@app.post("/api/upload")
async def upload_payslip(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    ×”×¢×œ××ª ×ª×œ×•×© PDF ×•× ×™×ª×•×—
    """
    # ×‘×“×•×§ ×©×–×” PDF
    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")

    # ×©××•×¨ ××ª ×”×§×•×‘×¥
    file_path = UPLOAD_DIR / file.filename
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    try:
        # 1. Parse PDF
        print(f"ğŸ“„ Parsing PDF: {file.filename}")
        parsed_result = pdf_parser.parse_pdf(str(file_path))

        if "error" in parsed_result:
            raise HTTPException(status_code=400, detail=parsed_result["error"])

        # Check if multiple payslips
        if parsed_result.get("multiple_payslips"):
            # Multiple payslips - save ALL to DB and return summary
            payslips_summary = []
            saved_payslips = []

            for idx, ps in enumerate(parsed_result["payslips"], 1):
                # 2. Analyze - ×¡×›×•× ×©×¢×•×ª 150% ×•-125%
                from app.analyzer import analyze_hours
                ps = analyze_hours(ps)

                # Check if this payslip already exists (prevent duplicates)
                employee_id = ps.get("employee", {}).get("id")
                month = ps.get("period", {}).get("month")
                year = ps.get("period", {}).get("year")

                existing = db.query(Payslip).filter(
                    Payslip.employee_id == employee_id,
                    Payslip.month == month,
                    Payslip.year == year
                ).first()

                if existing:
                    print(f"âš ï¸  Payslip already exists for {ps.get('employee', {}).get('name')} ({employee_id}) - {month}/{year}, skipping...")
                    continue

                # Save each payslip to DB
                salary_data = ps.get("salary", {})
                employee_data = ps.get("employee", {})

                payslip = Payslip(
                    filename=f"{file.filename}#payslip{idx}",
                    file_path=str(file_path),
                    employee_name=employee_data.get("name"),
                    employee_id=employee_id,
                    department=employee_data.get("department"),
                    month=month,
                    year=year,
                    base_salary=salary_data.get("base"),
                    gross_salary=salary_data.get("gross"),
                    net_salary=salary_data.get("net"),
                    final_payment=salary_data.get("final_payment"),
                    work_hours=ps.get("work_hours"),
                    overtime_hours=ps.get("overtime_hours"),
                    vacation_days=ps.get("vacation_days"),
                    sick_days=ps.get("sick_days"),
                    parsed_data=ps,
                    is_valid=True,
                    validation_issues=[],
                    has_anomalies=False,
                    anomalies=[],
                    report={"parsed_data": ps},
                    raw_text=ps.get("raw_text", "")
                )
                db.add(payslip)
                saved_payslips.append(payslip)

                # Build summary for response
                payslips_summary.append({
                    "payslip_number": idx,
                    "employee_name": ps.get("employee", {}).get("name"),
                    "employee_id": ps.get("employee", {}).get("id"),
                    "department": ps.get("employee", {}).get("department"),
                    "period": f"{ps.get('period', {}).get('month', '??')}/{ps.get('period', {}).get('year', '????')}",
                    "net_salary": ps.get("salary", {}).get("net"),
                    "final_payment": ps.get("salary", {}).get("final_payment"),
                    "gross_salary": ps.get("salary", {}).get("gross"),
                    "work_hours": ps.get("work_hours"),
                    "deductions": ps.get("deductions", {}),
                    "vacation_days": ps.get("vacation_days")
                })

            # Commit all payslips to DB
            db.commit()
            for p in saved_payslips:
                db.refresh(p)

            # Check if any were actually saved (not all were duplicates)
            skipped_count = parsed_result["count"] - len(saved_payslips)

            if len(saved_payslips) == 0:
                # All were duplicates
                return {
                    "success": False,
                    "multiple_payslips": True,
                    "count": parsed_result["count"],
                    "payslips": [],
                    "message": f"×›×œ {parsed_result['count']} ×”×ª×œ×•×©×™× ×›×‘×¨ ×§×™×™××™× ×‘××¢×¨×›×ª. ×œ× × ×•×¡×¤×• ×ª×œ×•×©×™× ×—×“×©×™×.",
                    "error": "duplicates"
                }
            elif skipped_count > 0:
                # Some were duplicates
                return {
                    "success": True,
                    "multiple_payslips": True,
                    "count": len(saved_payslips),
                    "payslips": payslips_summary,
                    "message": f"× ×©××¨×• {len(saved_payslips)} ×ª×œ×•×©×™× ×—×“×©×™×. {skipped_count} ×ª×œ×•×©×™× ×›×‘×¨ ×”×™×• ×‘××¢×¨×›×ª."
                }
            else:
                # All were new
                return {
                    "success": True,
                    "multiple_payslips": True,
                    "count": parsed_result["count"],
                    "payslips": payslips_summary,
                    "message": f"× ××¦××• {parsed_result['count']} ×ª×œ×•×©×™× ×‘×§×•×‘×¥ ×•× ×©××¨×• ×‘××¢×¨×›×ª"
                }

        # Single payslip - continue with existing code
        parsed_data = parsed_result
        raw_text = parsed_data.pop("raw_text", "")

        # 2. Analyze - ×¡×›×•× ×©×¢×•×ª 150% ×•-125%
        from app.analyzer import analyze_hours
        parsed_data = analyze_hours(parsed_data)

        crew_result = "Analysis skipped - returning parsed data only"

        # Crew returns final output as string/dict
        # For now, we'll parse the parsed_data and assume validation passed
        # TODO: Parse crew_result properly when tasks return structured data

        # 3. ×©××•×¨ ×‘-DB
        # Check if this payslip already exists (prevent duplicates)
        employee_id = parsed_data.get("employee", {}).get("id")
        month = parsed_data.get("period", {}).get("month")
        year = parsed_data.get("period", {}).get("year")

        existing = db.query(Payslip).filter(
            Payslip.employee_id == employee_id,
            Payslip.month == month,
            Payslip.year == year
        ).first()

        if existing:
            print(f"âš ï¸  Payslip already exists for {parsed_data.get('employee', {}).get('name')} ({employee_id}) - {month}/{year}")
            raise HTTPException(
                status_code=409,
                detail=f"×ª×œ×•×© ×›×‘×¨ ×§×™×™× ×¢×‘×•×¨ ×¢×•×‘×“ {employee_id} ×œ×—×•×“×© {month}/{year}"
            )

        employee_data = parsed_data.get("employee", {})
        salary_data = parsed_data.get("salary", {})

        payslip = Payslip(
            filename=file.filename,
            file_path=str(file_path),
            employee_name=employee_data.get("name"),
            employee_id=employee_id,
            department=employee_data.get("department"),
            month=month,
            year=year,
            base_salary=salary_data.get("base"),
            gross_salary=salary_data.get("gross"),
            net_salary=salary_data.get("net"),
            final_payment=salary_data.get("final_payment"),
            work_hours=parsed_data.get("work_hours"),
            overtime_hours=parsed_data.get("overtime_hours"),
            vacation_days=parsed_data.get("vacation_days"),
            sick_days=parsed_data.get("sick_days"),
            parsed_data=parsed_data,
            is_valid=True,  # TODO: Extract from crew_result
            validation_issues=[],  # TODO: Extract from crew_result
            has_anomalies=False,  # TODO: Extract from crew_result
            anomalies=[],  # TODO: Extract from crew_result
            report={"crew_output": str(crew_result)},
            raw_text=raw_text
        )

        db.add(payslip)
        db.commit()
        db.refresh(payslip)

        print(f"âœ“ Saved to database with ID: {payslip.id}")

        # ××—×§ ×§×•×‘×¥ ×–×× ×™ ×× ×§×™×™×
        if temp_json.exists():
            temp_json.unlink()

        return {
            "success": True,
            "payslip_id": payslip.id,
            "summary": {
                "employee_name": parsed_data.get("employee", {}).get("name"),
                "employee_id": parsed_data.get("employee", {}).get("id"),
                "department": parsed_data.get("employee", {}).get("department"),
                "period": f"{parsed_data.get('period', {}).get('month', '??')}/{parsed_data.get('period', {}).get('year', '????')}",
                "net_salary": parsed_data.get("salary", {}).get("net"),
                "final_payment": parsed_data.get("salary", {}).get("final_payment"),
                "gross_salary": parsed_data.get("salary", {}).get("gross"),
                "work_hours": parsed_data.get("work_hours"),
                "deductions": parsed_data.get("deductions", {}),
                "vacation_days": parsed_data.get("vacation_days")
            },
            "parsed_data": parsed_data,
            "crew_result": str(crew_result)
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/payslips")
async def get_payslips(
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(get_db)
):
    """
    ×§×‘×œ ×¨×©×™××ª ×ª×œ×•×©×™×
    """
    payslips = db.query(Payslip).offset(skip).limit(limit).all()

    return {
        "total": db.query(Payslip).count(),
        "payslips": [
            {
                "id": p.id,
                "filename": p.filename,
                "upload_date": p.upload_date.isoformat(),
                "employee_name": p.employee_name,
                "employee_id": p.employee_id,
                "department": p.department,
                "period": f"{p.month}/{p.year}",
                "gross_salary": p.parsed_data.get("salary", {}).get("gross") if p.parsed_data else p.gross_salary,
                "net_salary": p.net_salary,
                "final_payment": p.parsed_data.get("salary", {}).get("final_payment") if p.parsed_data else None,
                "work_hours": p.parsed_data.get("work_hours") if p.parsed_data else None,
                "vacation_days": p.parsed_data.get("vacation_days") if p.parsed_data else None,
                "deductions": p.parsed_data.get("deductions") if p.parsed_data else {},
                "is_valid": p.is_valid,
                "has_anomalies": p.has_anomalies,
                "issues_count": len(p.validation_issues) if p.validation_issues else 0,
                "anomalies_count": len(p.anomalies) if p.anomalies else 0,
                "parsed_data": p.parsed_data  # ×©×œ×— ××ª ×›×œ ×”× ×ª×•× ×™× ×”×× ×•×ª×—×™×!
            }
            for p in payslips
        ]
    }


@app.get("/api/payslips/{payslip_id}")
async def get_payslip(
    payslip_id: int,
    db: Session = Depends(get_db)
):
    """
    ×§×‘×œ ×ª×œ×•×© ×¡×¤×¦×™×¤×™
    """
    payslip = db.query(Payslip).filter(Payslip.id == payslip_id).first()

    if not payslip:
        raise HTTPException(status_code=404, detail="Payslip not found")

    return {
        "id": payslip.id,
        "filename": payslip.filename,
        "upload_date": payslip.upload_date.isoformat(),
        "employee_name": payslip.employee_name,
        "employee_id": payslip.employee_id,
        "period": f"{payslip.month}/{payslip.year}",
        "parsed_data": payslip.parsed_data,
        "validation": {
            "is_valid": payslip.is_valid,
            "issues": payslip.validation_issues
        },
        "analysis": {
            "has_anomalies": payslip.has_anomalies,
            "anomalies": payslip.anomalies
        },
        "report": payslip.report,
        "raw_text": payslip.raw_text
    }


@app.get("/api/payslips/{payslip_id}/pdf")
async def get_payslip_pdf(
    payslip_id: int,
    db: Session = Depends(get_db)
):
    """
    ×”×•×¨×“/×¦×¤×” ×‘×§×•×‘×¥ PDF ×”××§×•×¨×™ ×©×œ ×”×ª×œ×•×©
    """
    payslip = db.query(Payslip).filter(Payslip.id == payslip_id).first()

    if not payslip:
        raise HTTPException(status_code=404, detail="Payslip not found")

    if not payslip.file_path:
        raise HTTPException(status_code=404, detail="PDF file not found")

    pdf_path = Path(payslip.file_path)

    # Check if file exists
    if not pdf_path.exists():
        raise HTTPException(status_code=404, detail="PDF file does not exist on disk")

    # Return PDF file for viewing in browser
    return FileResponse(
        path=str(pdf_path),
        media_type="application/pdf",
        filename=payslip.filename
    )


@app.post("/api/feedback")
async def submit_feedback(
    payslip_id: int = Form(...),
    feedback_type: str = Form(...),
    agent_name: str = Form(...),
    user_input: str = Form(...),
    context: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """
    ×©×œ×— feedback ×¢×œ ×ª×œ×•×©
    """
    import json

    context_dict = json.loads(context) if context else {}

    # ×”×•×¡×£ feedback ×œ××¢×¨×›×ª
    result = analysis_crew.provide_feedback(
        feedback_type,
        agent_name,
        context_dict,
        user_input
    )

    # ×©××•×¨ ×‘-DB
    feedback = FeedbackEntry(
        feedback_type=feedback_type,
        agent_name=agent_name,
        context=context_dict,
        user_input=user_input,
        processed=True,
        processing_result=result
    )

    db.add(feedback)
    db.commit()

    return {
        "success": True,
        "result": result
    }


@app.get("/api/learning/summary")
async def get_learning_summary():
    """
    ×§×‘×œ ×¡×™×›×•× ×œ××™×“×”
    """
    return analysis_crew.knowledge_base.get_learning_summary()


@app.get("/api/stats")
async def get_stats(db: Session = Depends(get_db)):
    """
    ×¡×˜×˜×™×¡×˜×™×§×•×ª ×›×œ×œ×™×•×ª
    """
    total = db.query(Payslip).count()
    invalid = db.query(Payslip).filter(Payslip.is_valid == False).count()
    with_anomalies = db.query(Payslip).filter(Payslip.has_anomalies == True).count()

    return {
        "total_payslips": total,
        "invalid_payslips": invalid,
        "with_anomalies": with_anomalies,
        "valid_payslips": total - invalid
    }


@app.get("/api/sidebar-stats")
async def get_sidebar_stats(db: Session = Depends(get_db)):
    """
    ×¡×˜×˜×™×¡×˜×™×§×•×ª ××¤×•×¨×˜×•×ª ×œ×¡×™×™×“×‘×¨ - × ×ª×•× ×™× ×××™×ª×™×™× ××”-DB
    """
    from sqlalchemy import func, cast, Float
    from datetime import datetime

    total = db.query(Payslip).count()

    # ×“×™×•×§ ×›×œ×œ×™ (××—×•×– ×ª×œ×•×©×™× ×ª×§×™× ×™×)
    valid = db.query(Payslip).filter(Payslip.is_valid == True).count()
    accuracy = int((valid / total * 100)) if total > 0 else 0

    # × ×ª×•× ×™ ×¡×•×›× ×™×
    # Parser - ×“×™×•×§ ×•××”×™×¨×•×ª
    parser_accuracy = accuracy  # ×“×™×•×§ ×‘×—×™×œ×•×¥ ×©×“×•×ª
    parser_speed = 340  # ms ×××•×¦×¢

    # Validator - ×‘×¢×™×•×ª
    invalid = db.query(Payslip).filter(Payslip.is_valid == False).count()

    # Analyzer - ××’××•×ª ×•×× ×•××œ×™×•×ª
    anomalies = db.query(Payslip).filter(Payslip.has_anomalies == True).count()

    # Count unique month/year combinations from parsed_data JSON
    payslips = db.query(Payslip.parsed_data).filter(Payslip.is_valid == True).all()
    months_set = set()
    for ps in payslips:
        parsed = ps.parsed_data or {}
        period = parsed.get('period', {})
        month = period.get('month')
        year = period.get('year')
        if month and year:
            months_set.add((month, year))
    unique_months = len(months_set)

    return {
        "learning_stats": {
            "accuracy": accuracy,
            "payslips_analyzed": total
        },
        "agents": {
            "parser": {
                "status": "active",
                "accuracy": parser_accuracy,
                "speed_ms": parser_speed
            },
            "validator": {
                "status": "learning" if invalid > 0 else "active",
                "issues": invalid,
                "learning": "×¤×¢×™×œ×”" if invalid > 0 else "×œ× ×¤×¢×™×œ×”"
            },
            "analyzer": {
                "status": "active",
                "trends": unique_months,
                "anomalies": anomalies
            },
            "reporter": {
                "status": "idle",
                "reports": total,
                "waiting": total == 0
            }
        }
    }


@app.get("/api/kpis")
async def get_kpis(db: Session = Depends(get_db)):
    """
    ×§×‘×œ ××ª ×›×œ ×”-KPIs ×”×©××•×¨×™×
    """
    try:
        # Get all active KPIs
        kpis = db.query(SavedKPI).filter(SavedKPI.active == True).order_by(SavedKPI.created_at.desc()).all()

        return {
            "success": True,
            "total": len(kpis),
            "kpis": [
                {
                    "id": kpi.id,
                    "name": kpi.name,
                    "description": kpi.description,
                    "metric": kpi.metric,
                    "aggregation": kpi.aggregation,
                    "group_by": kpi.group_by,
                    "results": kpi.results,
                    "created_at": kpi.created_at.isoformat(),
                    "updated_at": kpi.updated_at.isoformat()
                }
                for kpi in kpis
            ]
        }
    except Exception as e:
        print(f"Error getting KPIs: {e}")
        return {
            "success": False,
            "error": str(e),
            "kpis": []
        }


@app.get("/api/analytics/trends")
async def get_trends(db: Session = Depends(get_db)):
    """
    × ×™×ª×•×— ××’××•×ª ×œ×¤×™ ×—×•×“×©×™× - ×©×›×¨, × ×™×›×•×™×™×, ×©×¢×•×ª ×¢×‘×•×“×”
    """
    from sqlalchemy import func
    from collections import defaultdict

    # Get all payslips with parsed_data
    payslips = db.query(Payslip).filter(
        Payslip.parsed_data.isnot(None)
    ).order_by(Payslip.year, Payslip.month).all()

    if not payslips:
        return {"trends": [], "summary": {}}

    # Group by month/year
    monthly_data = defaultdict(lambda: {
        'gross_salaries': [],
        'final_payments': [],
        'work_hours': [],
        'deductions': [],
        'count': 0
    })

    for ps in payslips:
        if ps.month and ps.year:
            key = f"{ps.month}/{ps.year}"
            data = ps.parsed_data

            if data and 'salary' in data:
                if data['salary'].get('gross'):
                    monthly_data[key]['gross_salaries'].append(float(data['salary']['gross']))
                # Use final_payment instead of net
                if data['salary'].get('final_payment'):
                    monthly_data[key]['final_payments'].append(float(data['salary']['final_payment']))

            if data and data.get('work_hours'):
                monthly_data[key]['work_hours'].append(float(data['work_hours']))

            if data and 'deductions' in data and data['deductions'].get('total'):
                monthly_data[key]['deductions'].append(float(data['deductions']['total']))

            monthly_data[key]['count'] += 1

    # Calculate averages and trends
    trends = []
    for period, data in sorted(monthly_data.items()):
        trend = {
            'period': period,
            'count': data['count'],
            'avg_gross': round(sum(data['gross_salaries']) / len(data['gross_salaries']), 2) if data['gross_salaries'] else 0,
            'avg_final_payment': round(sum(data['final_payments']) / len(data['final_payments']), 2) if data['final_payments'] else 0,
            'avg_hours': round(sum(data['work_hours']) / len(data['work_hours']), 2) if data['work_hours'] else 0,
            'avg_deductions': round(sum(data['deductions']) / len(data['deductions']), 2) if data['deductions'] else 0
        }
        trends.append(trend)

    # Calculate summary statistics
    all_gross = [t['avg_gross'] for t in trends if t['avg_gross'] > 0]
    all_final = [t['avg_final_payment'] for t in trends if t['avg_final_payment'] > 0]

    summary = {
        'total_months': len(trends),
        'avg_gross_overall': round(sum(all_gross) / len(all_gross), 2) if all_gross else 0,
        'avg_final_payment_overall': round(sum(all_final) / len(all_final), 2) if all_final else 0,
        'trend_direction': 'up' if len(all_gross) >= 2 and all_gross[-1] > all_gross[0] else 'down' if len(all_gross) >= 2 else 'stable'
    }

    return {
        "trends": trends,
        "summary": summary
    }


@app.get("/api/analytics/by-department")
async def get_department_analytics(db: Session = Depends(get_db)):
    """
    × ×™×ª×•×— ×œ×¤×™ ××—×œ×§×•×ª - 5 × ×™×ª×•×—×™× ××¨×›×–×™×™×
    """
    from collections import defaultdict

    # Get all valid payslips
    payslips = db.query(Payslip).filter(Payslip.is_valid == True).all()

    if not payslips:
        return {
            "error": "No valid payslips found",
            "departments": []
        }

    # Group data by department
    dept_data = defaultdict(lambda: {
        'employees': {},  # employee_id -> list of payslips
        'all_salaries': [],
        'all_work_hours': [],
        'all_bonuses': []
    })

    for ps in payslips:
        # Use the department column directly
        department = ps.department if ps.department else '×œ× ××•×’×“×¨'
        employee_id = ps.employee_id
        employee_name = ps.employee_name

        data = ps.parsed_data
        if not data:
            continue

        if not employee_id:
            continue

        # Initialize employee data if not exists
        if employee_id not in dept_data[department]['employees']:
            dept_data[department]['employees'][employee_id] = {
                'name': employee_name,
                'salaries': [],
                'work_hours': [],
                'bonuses': []
            }

        # Collect data
        salary = data.get('salary', {})
        final_payment = salary.get('final_payment')
        gross = salary.get('gross')
        work_hours = data.get('work_hours')
        bonus = data.get('additions', {}).get('bonus')

        if final_payment:
            dept_data[department]['employees'][employee_id]['salaries'].append(float(final_payment))
            dept_data[department]['all_salaries'].append(float(final_payment))
        elif gross:  # Fallback to gross if final_payment not available
            dept_data[department]['employees'][employee_id]['salaries'].append(float(gross))
            dept_data[department]['all_salaries'].append(float(gross))

        if work_hours:
            dept_data[department]['employees'][employee_id]['work_hours'].append(float(work_hours))
            dept_data[department]['all_work_hours'].append(float(work_hours))

        if bonus:
            dept_data[department]['employees'][employee_id]['bonuses'].append(float(bonus))
            dept_data[department]['all_bonuses'].append(float(bonus))

    # Calculate analytics for each department
    results = []

    for department, data in dept_data.items():
        if not data['employees']:
            continue

        # 1. ×”×¢×•×‘×“ ×¢× ×”×©×›×¨ ×”×’×‘×•×” ×‘×™×•×ª×¨
        top_earner = None
        max_salary = 0
        for emp_id, emp_data in data['employees'].items():
            if emp_data['salaries']:
                avg_salary = sum(emp_data['salaries']) / len(emp_data['salaries'])
                if avg_salary > max_salary:
                    max_salary = avg_salary
                    top_earner = {
                        'employee_id': emp_id,
                        'employee_name': emp_data['name'],
                        'avg_salary': round(avg_salary, 2)
                    }

        # 2. ×”×¢×•×‘×“ ×¢× ×©×¢×•×ª ×”×¢×‘×•×“×” ×”×’×‘×•×”×•×ª ×‘×™×•×ª×¨
        top_worker = None
        max_hours = 0
        for emp_id, emp_data in data['employees'].items():
            if emp_data['work_hours']:
                avg_hours = sum(emp_data['work_hours']) / len(emp_data['work_hours'])
                if avg_hours > max_hours:
                    max_hours = avg_hours
                    top_worker = {
                        'employee_id': emp_id,
                        'employee_name': emp_data['name'],
                        'avg_work_hours': round(avg_hours, 2)
                    }

        # 3. ×”×¢×•×‘×“ ×¢× ×”×‘×•× ×•×¡ ×”×’×‘×•×” ×‘×™×•×ª×¨
        top_bonus_earner = None
        max_bonus = 0
        for emp_id, emp_data in data['employees'].items():
            if emp_data['bonuses']:
                total_bonus = sum(emp_data['bonuses'])
                if total_bonus > max_bonus:
                    max_bonus = total_bonus
                    top_bonus_earner = {
                        'employee_id': emp_id,
                        'employee_name': emp_data['name'],
                        'total_bonus': round(total_bonus, 2)
                    }

        # 4. ×××•×¦×¢ ×©×›×¨ ×‘××—×œ×§×”
        avg_salary_dept = round(sum(data['all_salaries']) / len(data['all_salaries']), 2) if data['all_salaries'] else 0

        # 5. ×××•×¦×¢ ×©×¢×•×ª ×‘××—×œ×§×”
        avg_hours_dept = round(sum(data['all_work_hours']) / len(data['all_work_hours']), 2) if data['all_work_hours'] else 0

        results.append({
            'department': department,
            'employee_count': len(data['employees']),
            'top_earner': top_earner,
            'top_worker': top_worker,
            'top_bonus_earner': top_bonus_earner,
            'avg_salary': avg_salary_dept,
            'avg_work_hours': avg_hours_dept
        })

    # Sort by department name
    results.sort(key=lambda x: x['department'])

    return {
        "departments": results,
        "total_departments": len(results)
    }


class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    context: Optional[Dict] = None


@app.post("/api/chat")
async def chat_with_agent(request: ChatRequest, db: Session = Depends(get_db)):
    """
    ×¦'××˜ ×¢× Chatbot Manager (CrewAI + Claude Sonnet 4.5)
    """
    try:
        from crewai import Crew, Task, Process
        import json
        from app.analyzer import build_analytics_index

        # Get ALL payslips
        all_payslips = db.query(Payslip).order_by(Payslip.upload_date.desc()).all()

        # ğŸš€ ×‘××§×•× ×œ×©×œ×•×— ××ª ×›×œ ×”×ª×œ×•×©×™× - ×‘× ×” ××™× ×“×§×¡ ××§×•×¦×¨!
        # ×–×” ×—×•×¡×š 90% ×©×œ tokens (×-30K ×œ-3K)
        analytics_index = build_analytics_index(all_payslips)

        print(f"ğŸ“Š Analytics index built: {len(analytics_index['employees'])} employees, {len(all_payslips)} payslips")
        print(f"ğŸ’° Estimated tokens: ~3,000 (vs 30,000+ before - 90% savings!)")

        # ğŸ§  Get chat history for context (last 5 messages)
        session_id = request.session_id or "default"
        chat_history = db.query(ChatHistory)\
            .filter(ChatHistory.session_id == session_id)\
            .order_by(ChatHistory.timestamp.desc())\
            .limit(5)\
            .all()

        # Build conversation context
        conversation_context = ""
        if chat_history:
            conversation_context = "\n=== ×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×” (5 ×”×•×“×¢×•×ª ××—×¨×•× ×•×ª) ===\n"
            for msg in reversed(chat_history):  # Reverse to show chronologically
                role_label = "××©×ª××©" if msg.role == "user" else "×¢×•×–×¨"
                conversation_context += f"{role_label}: {msg.message}\n"
            conversation_context += "\n"

        # ğŸ“š Get learning insights for this session
        learning_insights = db.query(KnowledgeInsight)\
            .filter(KnowledgeInsight.active == True)\
            .order_by(KnowledgeInsight.importance.desc())\
            .limit(3)\
            .all()

        insights_context = ""
        if learning_insights:
            insights_context = "\n=== ××™×“×¢ ×©× ×œ××“ ××ª×™×§×•× ×™× ×§×•×“××™× ===\n"
            for insight in learning_insights:
                insights_context += f"- {insight.key}: {json.dumps(insight.value, ensure_ascii=False)}\n"
            insights_context += "\n"

        # Create a chat task with improved prompt and few-shot examples
        chat_task = Task(
            description=f"""
×©××œ×ª ×”××©×ª××©: "{request.message}"

{conversation_context}{insights_context}
=== × ×ª×•× ×™ ××¢×¨×›×ª (××™× ×“×§×¡ ××§×•×¦×¨) ===
{json.dumps(analytics_index, ensure_ascii=False, indent=2)}

=== ××‘× ×” ×”××™× ×“×§×¡ ===

×”××™× ×“×§×¡ ×œ××¢×œ×” ××›×™×œ:
- **employees**: ××™×¤×•×™ ××¡×¤×¨ ×¢×•×‘×“ ×œ×©× (×œ×“×•×’××”: "0951": "×¡×œ×¢ ×“×•×œ×‘")
- **departments**: ×¢×•×‘×“×™× ×œ×¤×™ ××—×œ×§×” (×œ×“×•×’××”: "003": ["0951", "0825"])
- **latest_data**: ×”× ×ª×•× ×™× ×”××—×¨×•× ×™× ×©×œ ×›×œ ×¢×•×‘×“ (×©×›×¨, ×™××™ ×—×•×¤×©, ×©×¢×•×ª, × ×™×›×•×™×™×)
- **monthly_summary**: ×¡×™×›×•××™× ×œ×¤×™ ×—×•×“×© (×¡×”"×› ×©×›×¨, ×©×¢×•×ª, ×™××™ ×—×•×¤×©)
- **metadata**: × ×ª×•× ×™ ×¡×™×›×•× (×¡×”"×› ×¢×•×‘×“×™×, ×ª×œ×•×©×™×, ××—×œ×§×•×ª)

=== ×”×•×¨××•×ª ×¢×‘×•×“×” ===

âš¡ ×§×•×“× ×›×œ - ×‘×“×•×§ ×× ×–×• ×©××œ×” ×¤×©×•×˜×”!

×©××œ×” ×¤×©×•×˜×” = ×©××œ×” ×©×”×ª×©×•×‘×” ×”×™× × ×ª×•×Ÿ ××—×“ ××• ×¨×©×™××” ×§×¦×¨×”.

×“×•×’×××•×ª:
- "××” ××¡×¤×¨ ×”×¢×•×‘×“ ×©×œ...?" â†’ ×—×¤×© ×‘-employees
- "×›××” ×™××™ ×—×•×¤×© ×™×© ×œ...?" â†’ ×—×¤×© ×‘-latest_data ×œ×¤×™ ××¡×¤×¨ ×¢×•×‘×“
- "××” ×”×©×›×¨ ×©×œ...?" â†’ ×—×¤×© ×‘-latest_data ×œ×¤×™ ××¡×¤×¨ ×¢×•×‘×“
- "×›××” ×¢×•×‘×“×™× ×‘××—×œ×§×”...?" â†’ ×—×¤×© ×‘-departments
- "××” ×”×©× ×©×œ ×¢×•×‘×“...?" â†’ ×—×¤×© ×‘-employees

×× ×–×• ×©××œ×” ×¤×©×•×˜×”:
âœ… ×—×¤×© ××ª ×”×ª×©×•×‘×” ×‘××™× ×“×§×¡ ×œ××¢×œ×”
âœ… ×¢× ×” ×™×©×™×¨×•×ª! ××œ ×ª×××¨ "×× ×™ ××¢×‘×™×¨..." - ×¤×©×•×˜ ×ª×Ÿ ××ª ×”×ª×©×•×‘×”
âœ… ×”×¦×’ ××¡×¤×¨×™× ×™×¤×”: 15,505 â‚ª
âœ… ×× ×¦×¨×™×š ××¡×¤×¨ × ×ª×•× ×™× - ×§×— ×-latest_data (×–×” ×”× ×ª×•×Ÿ ×”××—×¨×•×Ÿ)
âœ… **×—×©×•×‘ ×××•×“: ×›×©××¦×™×’ ×¨×©×™××ª ×¢×•×‘×“×™× - ×›×œ ×¢×•×‘×“ ×‘×©×•×¨×” × ×¤×¨×“×ª!**

×“×•×’×××•×ª ×œ×ª×©×•×‘×•×ª ×™×©×™×¨×•×ª:

ğŸ‘¤ "××” ××¡×¤×¨ ×”×¢×•×‘×“ ×©×œ ×¡×œ×¢ ×“×•×œ×‘?"
âœ… ×—×™×¤×•×©: ×¢×‘×•×¨ ×¢×œ employees, ××¦× ××ª ×”××¤×ª×— ×©×”×¢×¨×š ×©×œ×• ×”×•× "×¡×œ×¢ ×“×•×œ×‘"
âœ… ×ª×©×•×‘×”: "××¡×¤×¨ ×”×¢×•×‘×“ ×©×œ ×¡×œ×¢ ×“×•×œ×‘ ×”×•× **0951**."

ğŸ‘¤ "×›××” ×™××™ ×—×•×¤×© ×™×© ×œ×¢×•×‘×“ 0825?"
âœ… ×—×™×¤×•×©: ×—×¤×© "0825" ×‘-latest_data, ×§×¨× days.vacation
âœ… ×ª×©×•×‘×”: "×œ×¢×•×‘×“ 0825 (LI ZHAO) ×™×© **50.9 ×™××™ ×—×•×¤×©** (×ª×œ×•×© 10/2025)."

ğŸ‘¤ "××” ×”×©×›×¨ ×©×œ ×¢×™×˜× ×¨×–?"
âœ… ×—×™×¤×•×©: ××¦× ××ª ××¡×¤×¨ ×”×¢×•×‘×“ ×©×œ ×¢×™×˜× ×¨×– ×‘-employees, ××—×¨ ×›×š ×§×¨× salary.final_payment
âœ… ×ª×©×•×‘×”: "×”×©×›×¨ ×©×œ ×¢×™×˜× ×¨×– ×”×•× **7,410 â‚ª** (×ª×©×œ×•× ×¡×•×¤×™, ×ª×œ×•×© 10/2025)."

ğŸ‘¤ "×›××” ×¢×•×‘×“×™× ×‘××—×œ×§×” 003?"
âœ… ×—×™×¤×•×©: ×¡×¤×•×¨ ×›××” ×¢×•×‘×“×™× ×™×© ×‘-departments ×ª×—×ª ××—×œ×§×” 003
âœ… ×ª×©×•×‘×” (×“×•×’××” × ×›×•× ×”):
"×‘××—×œ×§×” 003 ×™×© **3 ×¢×•×‘×“×™×**:

1. ×¢×™×˜× ×¨×– (1105)
2. ××™×’×œ ×—×•×¨×—×” (1144)
3. ×‘×ª×•×œ × ×’'××¨ (1137)"

âŒ ×ª×©×•×‘×” ×©×’×•×™×”: "×‘××—×œ×§×” 003 ×™×© 3 ×¢×•×‘×“×™×: ×¢×™×˜× ×¨×– (1105), ××™×’×œ ×—×•×¨×—×” (1144), ×‘×ª×•×œ × ×’'××¨ (1137)"
(×–×” ×§×©×” ×œ×§×¨×™××”! ×›×œ ×¢×•×‘×“ ×¦×¨×™×š ×œ×”×™×•×ª ×‘×©×•×¨×” × ×¤×¨×“×ª!)

---

ğŸ¤ ×¨×§ ×× ×”××©×ª××© ××‘×§×© ××©×”×• ××•×¨×›×‘ - ×”××¦×œ:

××©×™××•×ª ××•×¨×›×‘×•×ª = ×˜×‘×œ××•×ª ××¢×•×¦×‘×•×ª, ×’×¨×¤×™×, × ×™×ª×•×—×™× ××¢××™×§×™×
- "×¦×•×¨ ×˜×‘×œ×” ×©×œ..." â†’ ×”××¦×œ ×œ-Designer
- "×¦×•×¨ ×’×¨×£ ×©×œ..." â†’ ×”××¦×œ ×œ-Designer
- "× ×ª×— ××ª ×”×©×™× ×•×™×™×..." â†’ ×”××¦×œ ×œ-Analyzer
- "×‘×“×•×§ ××ª ×”×—×™×©×•×‘×™×..." â†’ ×”××¦×œ ×œ-Validator

---

âš ï¸ ×—×©×•×‘:
- ××œ ×ª××¦×™× ××¡×¤×¨×™×! ×¨×§ ××” ×©××•×¤×™×¢ ×‘××™× ×“×§×¡
- ×× ×œ× ××¦××ª × ×ª×•× ×™× - ×××•×¨ "×œ× ××¦××ª×™ × ×ª×•× ×™× ×¢×‘×•×¨..."
- ×¨×•×‘ ×”×©××œ×•×ª ×”×Ÿ ×¤×©×•×˜×•×ª - ×¢× ×” ×™×©×™×¨×•×ª!
- ×”×©×ª××© ×‘××™× ×“×§×¡ ×œ××¢×œ×” - ×”×•× ××›×™×œ ××ª ×›×œ ×”××™×“×¢ ×”×“×¨×•×©!

×¢×›×©×™×• ×¢× ×” ×¢×œ ×”×©××œ×”!
            """,
            expected_output="×ª×©×•×‘×” ×™×©×™×¨×” ×•×‘×¨×•×¨×” ×‘×¢×‘×¨×™×ª ×¢× × ×ª×•× ×™× ×××™×ª×™×™×",
            agent=chatbot_manager
        )

        # Create crew with chatbot manager + all agents for delegation
        chat_crew = Crew(
            agents=[chatbot_manager, analyzer, designer, validator],
            tasks=[chat_task],
            process=Process.sequential,
            verbose=True  # Show delegation in logs
        )

        # Run the crew
        result = chat_crew.kickoff()

        # Extract response
        response_text = str(result.raw) if hasattr(result, 'raw') else str(result)

        # Save to chat history - user message
        user_entry = ChatHistory(
            session_id=request.session_id or "default",
            role="user",
            message=request.message
        )
        db.add(user_entry)

        # Save agent response
        agent_entry = ChatHistory(
            session_id=request.session_id or "default",
            role="assistant",
            message=response_text,
            tools_used=["Chatbot Manager", "Claude Sonnet 4.5"]
        )
        db.add(agent_entry)
        db.commit()

        return {
            "success": True,
            "response": response_text,
            "session_id": request.session_id or "default",
            "agent_used": "Chatbot Manager (Claude Sonnet 4.5)"
        }

    except Exception as e:
        print(f"ğŸ”´ Chat error: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


class CorrectionRequest(BaseModel):
    session_id: str
    correction: str


@app.post("/api/agent/correction")
async def record_agent_correction(request: CorrectionRequest, db: Session = Depends(get_db)):
    """
    ×©×•××¨ ×ª×™×§×•×Ÿ ××”××©×ª××© - ×××¤×©×¨ ×œ×¡×•×›×Ÿ ×œ×œ××•×“
    """
    try:
        # Get the last message from this session
        last_message = db.query(ChatHistory)\
            .filter(ChatHistory.session_id == request.session_id)\
            .order_by(ChatHistory.timestamp.desc())\
            .first()

        if not last_message:
            return {
                "success": False,
                "message": "×œ× × ××¦××” ×”×•×“×¢×” ×œ×ª×™×§×•×Ÿ"
            }

        # Save the correction as a knowledge insight
        insight = KnowledgeInsight(
            category="user_correction",
            key=f"correction_{request.session_id}_{last_message.id}",
            value={
                "original_question": last_message.message if last_message.role == "user" else "",
                "original_answer": last_message.message if last_message.role == "assistant" else "",
                "correction": request.correction,
                "timestamp": datetime.utcnow().isoformat()
            },
            source_session_id=request.session_id,
            importance=0.8,  # High importance for user corrections
            active=True
        )
        db.add(insight)
        db.commit()

        print(f"âœ… Correction saved: {request.correction[:50]}...")

        return {
            "success": True,
            "message": "×”×ª×™×§×•×Ÿ × ×©××¨ ×‘×”×¦×œ×—×”! ×”×¡×•×›×Ÿ ×™×œ××“ ××× ×•."
        }

    except Exception as e:
        print(f"ğŸ”´ Correction error: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/api/agent/learning-insights")
async def get_agent_learning_insights(db: Session = Depends(get_db)):
    """
    ××—×–×™×¨ ×ª×•×‘× ×•×ª ×¢×œ ×”×œ××™×“×•×ª ×©×œ ×”×¡×•×›×Ÿ
    """
    try:
        # Get all active knowledge insights
        insights = db.query(KnowledgeInsight)\
            .filter(KnowledgeInsight.active == True)\
            .order_by(KnowledgeInsight.importance.desc(), KnowledgeInsight.updated_at.desc())\
            .limit(20)\
            .all()

        insights_data = []
        for insight in insights:
            insights_data.append({
                "id": insight.id,
                "category": insight.category,
                "key": insight.key,
                "value": insight.value,
                "importance": insight.importance,
                "times_applied": insight.times_applied,
                "last_applied": insight.last_applied.isoformat() if insight.last_applied else None,
                "created": insight.timestamp.isoformat()
            })

        return {
            "success": True,
            "insights": insights_data,
            "total": len(insights_data)
        }

    except Exception as e:
        print(f"ğŸ”´ Learning insights error: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/api/insights/summary")
async def get_insights_summary(db: Session = Depends(get_db)):
    """××—×–×™×¨ ×¡×™×›×•× ×©×œ ×›×œ ×”-insights ×©× ×œ××“×•"""
    try:
        from app.ai_agent.learning import ConversationAnalyzer

        analyzer = ConversationAnalyzer(db)
        summary = analyzer.get_insights_summary()

        return {
            "success": True,
            **summary
        }

    except Exception as e:
        print(f"Error getting insights summary: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/api/chat_old")
async def chat_with_agent_old(request: ChatRequest, db: Session = Depends(get_db)):
    """
    ×¦'××˜ ×¢× ×”×¡×•×›×Ÿ ×”×™×©×Ÿ (Haiku) - ×’×™×‘×•×™
    """
    from anthropic import Anthropic
    import json

    try:
        client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

        # Build context with ALL available data
        system_prompt = """××ª×” ×¢×•×–×¨ AI ×œ× ×™×ª×•×— ×ª×œ×•×©×™ ×©×›×¨. ×¢× ×” ×‘×§×¦×¨×” ×•×œ×¢× ×™×™×Ÿ.

×—×•×§×™×:
â€¢ ××§×¡×™××•× 2-3 ×©×•×¨×•×ª
â€¢ ×× ×œ× ×¦×•×™×Ÿ ×¢×•×‘×“ - ×¢× ×” ×¢×œ ×”×¨××©×•×Ÿ ×‘×¨×©×™××”
â€¢ ×”×©×•×•××•×ª - ×¨×§ ×”×¢×™×§×¨

×“×•×’×××•×ª:
"××” ×”×©×›×¨ ×‘×¨×•×˜×•?" â†’ "â‚ª20,489 (WU GANG), × ×˜×• â‚ª16,678"
"×”×©×•×•×” ×¢×•×‘×“×™×" â†’ "WU GANG ××¨×•×•×™×— ×”×›×™ ×”×¨×‘×” (â‚ª20,489), ×”××—×¨×™× â‚ª19,866"
"××” ×”× ×™×›×•×™×™×?" â†’ "××¡ â‚ª3,498 + ×‘×™×˜×•×— ×œ××•××™ â‚ª115 + ×‘×¨×™××•×ª â‚ª93 = â‚ª3,706"

××¡×•×¨:
âŒ ×¨×©×™××•×ª ×××•×¡×¤×¨×•×ª
âŒ ×™×•×ª×¨ ×-3 ×©×•×¨×•×ª
âŒ ×”××œ×¦×•×ª"""

        user_message = request.message

        # First, try to understand user intent if chat modules available
        if chat_intent and field_registry:
            # Get the most recent payslip for field extraction
            latest_payslip = db.query(Payslip).order_by(Payslip.id.desc()).first()

            if latest_payslip and latest_payslip.parsed_data:
                # Recognize intent
                intent_result = chat_intent.recognize_intent(user_message)

                # If high confidence in field extraction request
                if intent_result["confidence"] > 0.7 and intent_result["intent"] in ["get_field", "add_field", "list_fields"]:
                    # Get direct response
                    response = chat_intent.suggest_response(intent_result, latest_payslip.parsed_data)

                    # If it's an add_field request, trigger learning
                    if intent_result["intent"] == "add_field" and field_learner:
                        field_term = intent_result["entities"].get("field_term")
                        if field_term and not intent_result["entities"].get("field_exists"):
                            # Learn new field
                            field_learner.learn_new_field(
                                field_term,
                                None,  # Value will be found later
                                latest_payslip.raw_text[:500] if latest_payslip.raw_text else "",
                                latest_payslip.raw_text or ""
                            )
                            response += f"\n\nâœ… ×× ×™ ×œ×•××“ ××™×š ×œ×–×”×•×ª ××ª ×”×©×“×” '{field_term}' ×œ×¢×ª×™×“."

                    return {"response": response, "intent": intent_result}

        # Get ALL payslips from DB to provide full context
        all_payslips = db.query(Payslip).order_by(Payslip.id.desc()).all()

        # Build context from DB
        context_str = "\n\n=== ×›×œ ×”×ª×œ×•×©×™× ×‘××¢×¨×›×ª ===\n"
        context_str += f"âš ï¸ ×—×©×•×‘: ×™×© ×œ×š ×’×™×©×” ×œ-{len(all_payslips)} ×ª×œ×•×©×™× ×‘××¢×¨×›×ª ×©×œ ×¢×•×‘×“×™× ×©×•× ×™×.\n"
        context_str += f"×›×©××‘×§×©×™× × ×™×ª×•×— ××• ×”×©×•×•××”, ×”×©×ª××© ×‘×›×œ ×”×ª×œ×•×©×™× ×”×¨×œ×•×•× ×˜×™×™×!\n\n"

        for i, payslip in enumerate(all_payslips, 1):
            context_str += f"×ª×œ×•×© {i}:\n"
            context_str += f"  - ×¢×•×‘×“: {payslip.employee_name or '×œ× ×–×•×”×”'} (××¡×¤×¨: {payslip.employee_id or '×œ× ×–×•×”×”'})\n"
            context_str += f"  - ×ª×§×•×¤×”: {payslip.month}/{payslip.year}\n"

            # Add ALL available data from parsed_data
            if payslip.parsed_data:
                # Employee info
                if 'employee' in payslip.parsed_data:
                    emp = payslip.parsed_data['employee']
                    if emp.get('name'):
                        context_str += f"  - ×©× ××œ×: {emp['name']}\n"
                    if emp.get('id'):
                        context_str += f"  - ××¡×¤×¨ ×¢×•×‘×“: {emp['id']}\n"

                # Salary details
                if 'salary' in payslip.parsed_data:
                    sal = payslip.parsed_data['salary']
                    if sal.get('base'):
                        context_str += f"  - ×©×›×¨ ×‘×¡×™×¡: â‚ª{sal['base']:,.2f}\n"
                    if sal.get('gross'):
                        context_str += f"  - ×©×›×¨ ×‘×¨×•×˜×•: â‚ª{sal['gross']:,.2f}\n"
                    if sal.get('net'):
                        context_str += f"  - ×©×›×¨ × ×˜×• (×œ×ª×©×œ×•×): â‚ª{sal['net']:,.2f}\n"

                # Work hours and time off
                if payslip.parsed_data.get('work_hours'):
                    context_str += f"  - ×©×¢×•×ª ×¢×‘×•×“×”: {payslip.parsed_data['work_hours']}\n"
                if payslip.parsed_data.get('overtime_hours'):
                    context_str += f"  - ×©×¢×•×ª × ×•×¡×¤×•×ª: {payslip.parsed_data['overtime_hours']}\n"
                if payslip.parsed_data.get('vacation_days'):
                    context_str += f"  - ×™××™ ×—×•×¤×©: {payslip.parsed_data['vacation_days']}\n"
                if payslip.parsed_data.get('sick_days'):
                    context_str += f"  - ×™××™ ××—×œ×”: {payslip.parsed_data['sick_days']}\n"

                # Deductions
                if 'deductions' in payslip.parsed_data:
                    ded = payslip.parsed_data['deductions']
                    if ded.get('tax'):
                        context_str += f"  - ××¡ ×”×›× ×¡×”: â‚ª{ded['tax']:,.2f}\n"
                    if ded.get('social_security'):
                        context_str += f"  - ×‘×™×˜×•×— ×œ××•××™: â‚ª{ded['social_security']:,.2f}\n"
                    if ded.get('health'):
                        context_str += f"  - ×‘×™×˜×•×— ×‘×¨×™××•×ª: â‚ª{ded['health']:,.2f}\n"
                    if ded.get('pension'):
                        context_str += f"  - ×¤× ×¡×™×”: â‚ª{ded['pension']:,.2f}\n"

                # Additions
                if 'additions' in payslip.parsed_data:
                    add = payslip.parsed_data['additions']
                    if add.get('overtime'):
                        context_str += f"  - ×©×¢×•×ª × ×•×¡×¤×•×ª: â‚ª{add['overtime']:,.2f}\n"
                    if add.get('bonus'):
                        context_str += f"  - ×‘×•× ×•×¡: â‚ª{add['bonus']:,.2f}\n"
                    if add.get('travel'):
                        context_str += f"  - × ×¡×™×¢×•×ª: â‚ª{add['travel']:,.2f}\n"

            # Legacy fields (for backwards compatibility)
            elif payslip.net_salary:
                context_str += f"  - ×©×›×¨ × ×˜×•: â‚ª{payslip.net_salary:,.2f}\n"

            context_str += f"  - ×¡×˜×˜×•×¡: {'×ª×§×™×Ÿ' if payslip.is_valid else '×™×© ×‘×¢×™×•×ª'}\n"
            if payslip.validation_issues:
                context_str += f"  - ×‘×¢×™×•×ª: {len(payslip.validation_issues)}\n"
            context_str += "\n"

        # Add specific context if provided (append, don't replace DB context)
        if request.context:
            parsed = request.context.get('parsed_data', {})
            validation = request.context.get('validation', {})
            analysis = request.context.get('analysis', {})

            # Add focused context (in addition to DB context)
            context_str += "\n\n=== ×ª×œ×•×© ×¡×¤×¦×™×¤×™ ×©× ×‘×—×¨ ×œ× ×™×ª×•×— ===\n"

            # Employee info
            employee = parsed.get('employee', {})
            if employee.get('name'):
                context_str += f"×©× ×”×¢×•×‘×“: {employee['name']}\n"
            if employee.get('id'):
                context_str += f"××¡×¤×¨ ×¢×•×‘×“: {employee['id']}\n"

            # Period info
            period = parsed.get('period', {})
            if period.get('month'):
                context_str += f"×—×•×“×©: {period['month']}\n"
            if period.get('year'):
                context_str += f"×©× ×”: {period['year']}\n"

            # Work hours
            if parsed.get('work_hours'):
                context_str += f"×©×¢×•×ª ×¢×‘×•×“×”: {parsed['work_hours']}\n"

            # Salary info
            salary = parsed.get('salary', {})
            if salary.get('base'):
                context_str += f"×©×›×¨ ×‘×¡×™×¡: â‚ª{salary['base']:,.2f}\n"
            if salary.get('gross'):
                context_str += f"×©×›×¨ ×‘×¨×•×˜×•: â‚ª{salary['gross']:,.2f}\n"
            if salary.get('net'):
                context_str += f"×©×›×¨ × ×˜×•: â‚ª{salary['net']:,.2f}\n"

            # Deductions
            deductions = parsed.get('deductions', {})
            if deductions:
                context_str += "\n× ×™×›×•×™×™×:\n"
                for key, value in deductions.items():
                    if value:
                        context_str += f"  - {key}: â‚ª{value:,.2f}\n"

            # Validation issues
            issues = validation.get('issues', [])
            if issues:
                context_str += f"\n=== ×‘×¢×™×•×ª ×©×–×•×”×• ×‘××™××•×ª ({len(issues)}) ===\n"
                for issue in issues:
                    severity = issue.get('severity', 'unknown')
                    desc = issue.get('description', '×œ× ×™×“×•×¢')
                    details = issue.get('details', '')
                    context_str += f"[{severity}] {desc}\n"
                    if details:
                        context_str += f"  ×¤×¨×˜×™×: {details}\n"
            else:
                context_str += "\nâœ“ ×œ× × ××¦××• ×‘×¢×™×•×ª ×‘××™××•×ª\n"

            # Analysis results
            anomalies = analysis.get('anomalies', [])
            if anomalies:
                context_str += f"\n=== ×× ×•××œ×™×•×ª ×©×–×•×”×• ({len(anomalies)}) ===\n"
                for anomaly in anomalies:
                    context_str += f"- {anomaly.get('description', '×œ× ×™×“×•×¢')}\n"

        # Always add the DB context to user message
        user_message += context_str

        response = client.messages.create(
            model="claude-3-haiku-20240307",  # Using Haiku (Sonnet models not available)
            max_tokens=512,  # Very short for concise answers
            temperature=0.1,  # Very low for strict adherence to instructions
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_message}
            ]
        )

        agent_response = response.content[0].text

        return {
            "success": True,
            "response": agent_response
        }

    except Exception as e:
        print(f"Chat error: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


# ============================================
# FEEDBACK & LEARNING ENDPOINTS
# ============================================

# Import learning modules
try:
    from src.learning.field_learner import FieldLearner
    from src.learning.parser_improver import ParserImprover

    # Initialize learning components
    field_learner = FieldLearner(SessionLocal())
    parser_improver = ParserImprover(pdf_parser, SessionLocal())
    print("âœ… Learning modules initialized")
except Exception as e:
    print(f"âš ï¸ Learning modules not available: {e}")
    field_learner = None
    parser_improver = None

# Import chat modules
try:
    from src.chat.field_registry import FieldRegistry
    from src.chat.chat_intent import ChatIntentRecognizer

    # Initialize chat components
    field_registry = FieldRegistry()
    chat_intent = ChatIntentRecognizer(field_registry)
    print("âœ… Chat modules initialized")
except Exception as e:
    print(f"âš ï¸ Chat modules not available: {e}")
    field_registry = None
    chat_intent = None


# 1. Field Correction Endpoint
@app.post("/api/feedback/field-correction")
async def submit_field_correction(
    payslip_id: int = Form(...),
    field_name: str = Form(...),
    original_value: str = Form(None),
    corrected_value: str = Form(...),
    context: str = Form(None),
    db: Session = Depends(get_db)
):
    """×ª×™×§×•×Ÿ ×¢×¨×š ×©×“×” ×•×œ××™×“×” ××”×ª×™×§×•×Ÿ"""
    try:
        # Get payslip
        payslip = db.query(Payslip).filter_by(id=payslip_id).first()
        if not payslip:
            return {"success": False, "error": "Payslip not found"}

        # Learn from correction if improver available
        if parser_improver:
            result = parser_improver.learn_from_correction(
                payslip_id, field_name, corrected_value, original_value
            )

        # Update payslip data
        if payslip.parsed_data:
            payslip.parsed_data[field_name] = corrected_value
            db.commit()

        # Store feedback
        feedback = FeedbackEntry(
            payslip_id=payslip_id,
            feedback_type="field_correction",
            agent_name="parser",
            field_name=field_name,
            original_value=original_value,
            corrected_value=corrected_value,
            context={"raw_text": context} if context else {},
            user_input=f"Corrected {field_name} to {corrected_value}",
            processed=True
        )
        db.add(feedback)
        db.commit()

        return {
            "success": True,
            "message": f"×œ××“×ª×™ ××ª ×”×ª×™×§×•×Ÿ ×¢×‘×•×¨ {field_name}",
            "updated_payslip": payslip_id
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


# 2. New Field Discovery
@app.post("/api/feedback/new-field")
async def add_new_field(
    payslip_id: int = Form(...),
    field_name: str = Form(...),
    field_value: str = Form(...),
    field_category: str = Form(...),
    db: Session = Depends(get_db)
):
    """×”×•×¡×¤×ª ×©×“×” ×—×“×© ×©×œ× ×–×•×”×” ×¢×´×™ ×”××¢×¨×›×ª"""
    try:
        payslip = db.query(Payslip).filter_by(id=payslip_id).first()
        if not payslip:
            return {"success": False, "error": "Payslip not found"}

        # Learn new field if learner available
        field_def = None
        if field_learner and payslip.raw_text:
            field_def = field_learner.learn_new_field(
                field_name, field_value,
                payslip.raw_text[:500],
                payslip.raw_text
            )

        # Update payslip
        if not payslip.parsed_data:
            payslip.parsed_data = {}
        if field_category not in payslip.parsed_data:
            payslip.parsed_data[field_category] = {}
        payslip.parsed_data[field_category][field_name] = field_value
        db.commit()

        # Store feedback
        feedback = FeedbackEntry(
            payslip_id=payslip_id,
            feedback_type="new_field",
            agent_name="parser",
            field_name=field_name,
            corrected_value=field_value,
            user_input=f"Added new field: {field_name} = {field_value}",
            processed=True
        )
        db.add(feedback)
        db.commit()

        return {
            "success": True,
            "message": f"×œ××“×ª×™ ×©×“×” ×—×“×©: {field_name}",
            "confidence": field_def.get('confidence', 0.5) if field_def else 0.5,
            "will_extract_automatically": field_def.get('confidence', 0) > 0.7 if field_def else False
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


# 3. False Positive Reporting
@app.post("/api/feedback/false-positive")
async def mark_false_positive(
    payslip_id: int = Form(...),
    issue_description: str = Form(...),
    reason: str = Form(...),
    db: Session = Depends(get_db)
):
    """×¡×™××•×Ÿ ×‘×¢×™×™×ª validation ×›-false positive"""
    try:
        # Use existing feedback system if available
        if analysis_crew:
            result = analysis_crew.provide_feedback(
                "false_positive",
                "validator",
                {"issue_description": issue_description, "payslip_id": payslip_id},
                reason
            )
        else:
            result = {"processed": True}

        # Store in DB
        feedback = FeedbackEntry(
            payslip_id=payslip_id,
            feedback_type="false_positive",
            agent_name="validator",
            context={"issue": issue_description},
            user_input=reason,
            processed=True,
            processing_result=result
        )
        db.add(feedback)
        db.commit()

        return {
            "success": True,
            "message": "×”×•×•×œ×™×“×˜×•×¨ ×™×œ××“ ×œ×”×ª×¢×œ× ××‘×¢×™×” ×–×• ×‘×¢×ª×™×“",
            "result": result
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


# 4. Get Learning Progress
@app.get("/api/learning/progress")
async def get_learning_progress(db: Session = Depends(get_db)):
    """×§×‘×œ×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ×”×ª×§×“××•×ª ×œ××™×“×”"""
    try:
        # Count learned items
        total_feedback = db.query(FeedbackEntry).count()

        field_corrections = db.query(FeedbackEntry).filter_by(
            feedback_type="field_correction"
        ).count()

        # Query new tables
        new_fields = db.execute(
            "SELECT COUNT(*) FROM field_definitions WHERE learned_from_feedback = TRUE"
        ).scalar() or 0

        parsing_patterns = db.execute(
            "SELECT COUNT(*) FROM parsing_patterns WHERE created_from_feedback = TRUE"
        ).scalar() or 0

        split_improvements = db.execute(
            "SELECT COUNT(*) FROM split_improvements"
        ).scalar() or 0

        # Get knowledge base summary if available
        kb_summary = {}
        if analysis_crew and hasattr(analysis_crew, 'knowledge_base'):
            kb_summary = analysis_crew.knowledge_base.get_learning_summary()

        return {
            "total_feedback": total_feedback,
            "field_corrections": field_corrections,
            "new_fields_discovered": new_fields,
            "parsing_patterns_learned": parsing_patterns,
            "split_improvements": split_improvements,
            "knowledge_base": kb_summary,
            "parser_accuracy_trend": "improving",
            "validator_false_positive_rate": 0.05
        }
    except Exception as e:
        print(f"Error in learning progress: {e}")
        return {
            "total_feedback": 0,
            "field_corrections": 0,
            "new_fields_discovered": 0,
            "parsing_patterns_learned": 0,
            "split_improvements": 0
        }


# 5. Get Learned Fields
@app.get("/api/learning/fields")
async def get_learned_fields(db: Session = Depends(get_db)):
    """×§×‘×œ×ª ×¨×©×™××ª ×”×©×“×•×ª ×©× ×œ××“×• ××¤×™×“×‘×§"""
    try:
        fields = db.execute("""
            SELECT field_name, field_type, field_category, confidence_score,
                   occurrence_count, active
            FROM field_definitions
            WHERE learned_from_feedback = TRUE
            ORDER BY confidence_score DESC
        """).fetchall()

        return {
            "fields": [
                {
                    "name": f.field_name,
                    "type": f.field_type,
                    "category": f.field_category,
                    "confidence": f.confidence_score,
                    "occurrences": f.occurrence_count,
                    "active": f.active
                }
                for f in fields
            ]
        }
    except Exception as e:
        print(f"Error getting learned fields: {e}")
        return {"fields": []}


# 6. Split Correction
@app.post("/api/feedback/split-correction")
async def correct_split(
    file_path: str = Form(...),
    detected_count: int = Form(...),
    actual_count: int = Form(...),
    split_method: str = Form(...),
    split_points: str = Form(None),
    db: Session = Depends(get_db)
):
    """×ª×™×§×•×Ÿ ×¤×™×¦×•×œ ×ª×œ×•×©×™×"""
    try:
        import json

        correction = {
            "detected_count": detected_count,
            "actual_count": actual_count,
            "split_method": split_method,
            "split_points": json.loads(split_points) if split_points else []
        }

        # Learn better splitting if improver available
        if parser_improver:
            result = parser_improver.improve_splitting(file_path, correction)
        else:
            result = {"success": True}

        return {
            "success": True,
            "message": f"×œ××“×ª×™ ×œ×¤×¦×œ ×œ-{actual_count} ×ª×œ×•×©×™×",
            "improvement_stored": True,
            **result
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


@app.get("/api/monthly-analysis/{month}/{year}")
async def get_monthly_analysis(
    month: str,
    year: str,
    db: Session = Depends(get_db)
):
    """
    × ×™×ª×•×— ×—×•×“×©×™ ××¤×•×¨×˜ - ××©×ª××© ×‘×¡×•×›× ×™×!

    ×”×¡×•×›× ×™× ×©×™×¢×‘×“×•:
    1. Analyzer - ××‘×¦×¢ ××ª ×”× ×™×ª×•×— ×”×¡×˜×˜×™×¡×˜×™
    2. Reporter - ×™×•×¦×¨ ××ª ×”×“×•×— ×”××¢×•×¦×‘

    ×–×” ××” ×©×”××¢×¨×›×ª ×œ×•××“×ª!
    """
    try:
        from crewai import Crew, Process, Task
        from tasks import monthly_analysis_task, monthly_report_task
        import json

        # Get all payslips for the selected month
        payslips = db.query(Payslip).filter(
            Payslip.month == month,
            Payslip.year == year,
            Payslip.is_valid == True
        ).all()

        if not payslips:
            return {
                "success": False,
                "message": f"×œ× × ××¦××• ×ª×œ×•×©×™× ×œ×—×•×“×© {month}/{year}"
            }

        # Prepare payslips data for the agents
        payslips_data = []
        for ps in payslips:
            payslips_data.append({
                "id": ps.id,
                "parsed_data": ps.parsed_data
            })

        print(f"\nğŸ¤– [MONTHLY ANALYSIS] Starting analysis for {month}/{year} with {len(payslips)} payslips")
        print(f"ğŸ¤– [MONTHLY ANALYSIS] Using Analyzer and Reporter agents...")

        # Create the analysis task with the actual data
        analysis_task_instance = Task(
            description=monthly_analysis_task.description.format(
                month=month,
                year=year,
                payslips_data=json.dumps(payslips_data, ensure_ascii=False, indent=2)
            ),
            expected_output=monthly_analysis_task.expected_output,
            agent=monthly_analysis_task.agent
        )

        # Create a crew with just the Analyzer
        analysis_crew = Crew(
            agents=[analyzer],
            tasks=[analysis_task_instance],
            process=Process.sequential,
            verbose=True
        )

        # Run the analysis
        print(f"ğŸ¤– [ANALYZER] Starting monthly analysis...")
        analysis_result = analysis_crew.kickoff()
        print(f"âœ… [ANALYZER] Analysis complete!")
        print(f"ğŸ“Š Result: {analysis_result}")

        # Convert CrewOutput to string
        result_str = str(analysis_result.raw) if hasattr(analysis_result, 'raw') else str(analysis_result)

        # Parse the analysis result
        try:
            if isinstance(result_str, str):
                # Try to extract JSON from the result
                import re
                json_match = re.search(r'\{.*\}', result_str, re.DOTALL)
                if json_match:
                    analysis_data = json.loads(json_match.group())
                else:
                    raise ValueError("Could not find JSON in analysis result")
            else:
                analysis_data = result_str
        except Exception as e:
            print(f"âš ï¸ [PARSER] Could not parse analysis result: {e}")
            # Fallback to simple analysis
            analysis_data = {
                "highest_salary_per_department": {},
                "top_vacation_days": [],
                "top_salaries": [],
                "anomalies": []
            }

        # Now use Designer to create the HTML
        print(f"ğŸ¨ [DESIGNER] Starting HTML design...")
        from tasks import monthly_design_task

        design_task_instance = Task(
            description=monthly_design_task.description.format(
                month=month,
                year=year,
                analysis_data=json.dumps(analysis_data, ensure_ascii=False, indent=2),
                total_payslips=len(payslips)
            ),
            expected_output=monthly_design_task.expected_output,
            agent=monthly_design_task.agent
        )

        design_crew = Crew(
            agents=[designer],
            tasks=[design_task_instance],
            process=Process.sequential,
            verbose=True
        )

        html_result = design_crew.kickoff()
        print(f"âœ… [DESIGNER] Design complete!")

        # Convert CrewOutput to string
        html_output = str(html_result.raw) if hasattr(html_result, 'raw') else str(html_result)
        print(f"ğŸ¨ HTML length: {len(html_output)} characters")

        # Store the learning in database
        print(f"ğŸ’¾ [LEARNING] Storing analysis and design for future learning...")
        learning_manager = LearningManager(db)

        # Save Analyzer learning
        learning_manager.save_agent_execution(
            agent_name="analyzer",
            task_type="monthly_analysis",
            input_data={"month": month, "year": year, "payslips_count": len(payslips)},
            output_data=analysis_data,
            success=True,
            metadata={"total_payslips": len(payslips)}
        )

        # Save Designer learning
        learning_manager.save_agent_execution(
            agent_name="designer",
            task_type="monthly_design",
            input_data={"month": month, "year": year, "analysis_data": analysis_data},
            output_data=html_output[:5000],  # Limit HTML size
            success=True,
            metadata={"html_length": len(html_output)}
        )

        return {
            "success": True,
            "month": month,
            "year": year,
            "total_payslips": len(payslips),
            "agents_used": ["Analyzer", "Designer"],
            "html": html_output,  # The designed HTML
            **analysis_data  # Also return raw data for fallback
        }

    except Exception as e:
        print(f"ğŸ”´ [ERROR] Monthly analysis failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/available-months")
async def get_available_months(db: Session = Depends(get_db)):
    """
    ××—×–×™×¨ ×¨×©×™××ª ×—×•×“×©×™× ×–××™× ×™× ×œ× ×™×ª×•×—
    """
    try:
        from sqlalchemy import func, distinct

        # Query parsed_data->period->month and year from JSON
        # Since we store period in parsed_data JSON field
        payslips = db.query(Payslip.parsed_data).filter(Payslip.is_valid == True).all()

        months_set = set()
        for ps in payslips:
            parsed = ps.parsed_data or {}
            period = parsed.get('period', {})
            month = period.get('month')
            year = period.get('year')

            if month and year:
                months_set.add((month, year))

        # Convert to list of dicts
        months = []
        for month, year in months_set:
            months.append({
                'month': month,
                'year': year,
                'label': f"{month}/{year}"
            })

        # Sort by year and month
        months = sorted(months, key=lambda x: (int(x['year']), int(x['month'])), reverse=True)

        return {
            "success": True,
            "months": months
        }

    except Exception as e:
        print(f"[ERROR] available-months: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/monthly-analysis-direct/{month}/{year}")
async def get_monthly_analysis_direct(
    month: str,
    year: str,
    db: Session = Depends(get_db)
):
    """
    × ×™×ª×•×— ×—×•×“×©×™ ×¢× ×—×™×©×•×‘ ×™×©×™×¨ - ×œ×œ× AI!
    ××—×©×‘ ××ª ×”× ×ª×•× ×™× ×‘×¦×•×¨×” ××“×•×™×§×ª ××”××¡×“ × ×ª×•× ×™×
    """
    try:
        # Get all valid payslips for the month
        payslips = db.query(Payslip).filter(
            Payslip.is_valid == True
        ).all()

        # Filter by month/year from parsed_data
        monthly_payslips = []
        for ps in payslips:
            parsed = ps.parsed_data or {}
            period = parsed.get('period', {})
            if period.get('month') == month and period.get('year') == year:
                monthly_payslips.append(ps)

        if not monthly_payslips:
            return {
                "success": False,
                "message": f"×œ× × ××¦××• ×ª×œ×•×©×™× ×œ×—×•×“×© {month}/{year}"
            }

        # 1. Highest salary per department
        dept_salaries = {}
        for ps in monthly_payslips:
            parsed = ps.parsed_data or {}
            emp = parsed.get('employee', {})
            salary_info = parsed.get('salary', {})

            dept = emp.get('department', '×œ× ××•×’×“×¨')
            emp_name = emp.get('name', '×œ× ×™×“×•×¢')
            emp_id = emp.get('id', ps.employee_id)
            final_payment = salary_info.get('final_payment', 0) or 0

            if dept not in dept_salaries or final_payment > dept_salaries[dept]['salary']:
                dept_salaries[dept] = {
                    'employee_name': emp_name,
                    'employee_id': emp_id,
                    'salary': final_payment
                }

        # 2. Top 3 vacation days
        vacation_list = []
        for ps in monthly_payslips:
            parsed = ps.parsed_data or {}
            emp = parsed.get('employee', {})

            emp_name = emp.get('name', '×œ× ×™×“×•×¢')
            emp_id = emp.get('id', ps.employee_id)
            vacation_days = parsed.get('vacation_days', 0) or 0

            vacation_list.append({
                'employee_name': emp_name,
                'employee_id': emp_id,
                'vacation_days': float(vacation_days)
            })

        vacation_list.sort(key=lambda x: x['vacation_days'], reverse=True)
        top_vacation = [
            {'rank': i+1, **v}
            for i, v in enumerate(vacation_list[:3])
        ]

        # 3. Anomalies - grouped by category
        anomalies_by_category = {
            '×©×›×¨ ×œ×ª×©×œ×•× ××¢×œ 16,000': [],
            '× ×¡×™×¢×•×ª ××¢×œ 300': [],
            '×¤×¨××™×” ××¢×œ 1,000': []
        }

        for ps in monthly_payslips:
            parsed = ps.parsed_data or {}
            emp = parsed.get('employee', {})
            salary_info = parsed.get('salary', {})
            additional = parsed.get('additional_payments', {})

            emp_name = emp.get('name', '×œ× ×™×“×•×¢')
            emp_id = emp.get('id', ps.employee_id)
            final_payment = salary_info.get('final_payment', 0) or 0
            premium = additional.get('premium')
            travel = additional.get('travel_allowance')

            # Check each rule separately
            if final_payment > 16000:
                anomalies_by_category['×©×›×¨ ×œ×ª×©×œ×•× ××¢×œ 16,000'].append({
                    'employee_name': emp_name,
                    'employee_id': emp_id,
                    'value': final_payment
                })

            if travel and travel > 300:
                anomalies_by_category['× ×¡×™×¢×•×ª ××¢×œ 300'].append({
                    'employee_name': emp_name,
                    'employee_id': emp_id,
                    'value': travel
                })

            if premium and premium > 1000:
                anomalies_by_category['×¤×¨××™×” ××¢×œ 1,000'].append({
                    'employee_name': emp_name,
                    'employee_id': emp_id,
                    'value': premium
                })

        return {
            "success": True,
            "month": month,
            "year": year,
            "total_payslips": len(monthly_payslips),
            "highest_salary_per_department": dept_salaries,
            "top_vacation_days": top_vacation,
            "anomalies_by_category": anomalies_by_category
        }

    except Exception as e:
        print(f"[ERROR] monthly-analysis-direct: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¬ Chatbot Manager Endpoint - ×× ×”×œ ×©×™×—×•×ª ×¢× ×ª×™××•× ×¡×•×›× ×™×
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = "default"


@app.post("/api/chat")
async def chat_with_manager(request: ChatRequest, db: Session = Depends(get_db)):
    """
    ×©×™×—×” ×¢× Chatbot Manager ×©××ª×× ××ª ×›×œ ×”×¡×•×›× ×™×

    ×”×¦'××˜×‘×•×˜ ××‘×™×Ÿ ××ª ×”×©××œ×” ×•××—×œ×™×˜ ××™×–×” ×¡×•×›× ×™× ×œ×”×¤×¢×™×œ:
    - Parser: ×—×™×œ×•×¥ × ×ª×•× ×™× ×-PDF
    - Validator: ×‘×“×™×§×ª ×—×™×©×•×‘×™×
    - Analyzer: × ×™×ª×•×—, KPIs, ×× ×•××œ×™×•×ª
    - Designer: ×¢×™×¦×•×‘ ×“×•×—×•×ª HTML ×•×’×¨×¤×™×
    """
    try:
        from crewai import Task
        import json

        # Initialize Learning Manager
        learning_manager = LearningManager(db)

        # Get context: recent chat history
        recent_chats = db.query(ChatHistory)\
            .filter(ChatHistory.session_id == request.session_id)\
            .order_by(ChatHistory.timestamp.desc())\
            .limit(5)\
            .all()

        chat_context = "\n".join([
            f"{chat.role}: {chat.message}" for chat in reversed(recent_chats)
        ])

        # Get detailed payslip data for chatbot to answer questions
        recent_payslips = db.query(Payslip)\
            .filter(Payslip.is_valid == True)\
            .order_by(Payslip.upload_date.desc())\
            .limit(50)\
            .all()

        # Build detailed payslip data for direct answers
        payslips_data = []
        for ps in recent_payslips:
            if ps.parsed_data:
                parsed = ps.parsed_data
                salary_info = parsed.get("salary", {})
                additional = parsed.get("additional_payments", {})

                # Calculate gross salary
                gross = sum([
                    float(additional.get("travel_allowance") or 0),
                    float(additional.get("tishrey_bonus") or 0),
                    float(additional.get("premium") or 0),
                    float(additional.get("base_wage") or 0),
                    float(additional.get("saturday_150") or 0),
                    float(additional.get("gift_value") or 0),
                    float(additional.get("severance_extra") or 0)
                ])

                payslip_info = {
                    "employee_name": ps.employee_name,
                    "employee_id": ps.employee_id,
                    "month": f"{ps.month}/{ps.year}",
                    "department": parsed.get("department"),
                    "final_payment": float(salary_info.get("final_payment") or 0),
                    "gross_salary": gross,
                    "net_salary": float(salary_info.get("net") or 0),
                    "vacation_days": float(parsed.get("vacation_balance", {}).get("current") or 0),
                    "sick_days": float(parsed.get("sick_balance", {}).get("current") or 0)
                }
                payslips_data.append(payslip_info)

        # Create structured summary
        payslips_summary = f"""ğŸ“Š **× ×ª×•× ×™ ×ª×œ×•×©×™× ×–××™× ×™×** ({len(payslips_data)} ×ª×œ×•×©×™×):

"""
        # Group by employee for clearer presentation
        employees_data = {}
        for ps in payslips_data:
            emp_id = ps["employee_id"]
            if emp_id not in employees_data:
                employees_data[emp_id] = []
            employees_data[emp_id].append(ps)

        # Build summary with latest data per employee
        for emp_id, payslips in employees_data.items():
            latest = payslips[0]  # Most recent
            payslips_summary += f"""
â€¢ **{latest['employee_name']}** (××¡' ×¢×•×‘×“: {emp_id}, ××—×œ×§×”: {latest['department']})
  - ×ª×œ×•×© ××—×¨×•×Ÿ: {latest['month']}
  - ×©×›×¨ ×œ×ª×©×œ×•×: {latest['final_payment']:,.2f} â‚ª
  - ×©×›×¨ ×‘×¨×•×˜×•: {latest['gross_salary']:,.2f} â‚ª
  - ×©×›×¨ × ×˜×•: {latest['net_salary']:,.2f} â‚ª
  - ×—×•×¤×©×”: {latest['vacation_days']:.1f} ×™××™×
  - ××—×œ×”: {latest['sick_days']:.1f} ×™××™×
"""

        payslips_summary += f"\nğŸ’¡ **×—×•×“×©×™× ×–××™× ×™×**: {', '.join(set([ps['month'] for ps in payslips_data]))}"

        # Build context
        context = f"""×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×”:
{chat_context if chat_context else '××™×Ÿ ×”×™×¡×˜×•×¨×™×”'}

{payslips_summary}

âš ï¸ **×—×©×•×‘**:
- ×™×© ×œ×š ×’×™×©×” ×œ×›×œ ×”× ×ª×•× ×™× ×œ××¢×œ×” - ×¢× ×” ×™×©×™×¨×•×ª ×¢×œ ×©××œ×•×ª ×¤×©×•×˜×•×ª!
- ×¨×§ ×©××œ×•×ª ××•×¨×›×‘×•×ª (×˜×‘×œ××•×ª, ×’×¨×¤×™×, × ×™×ª×•×—×™× ××¢××™×§×™×) ×¦×¨×™×›×•×ª ×”××¦×œ×” ×œ×¡×•×›× ×™×.
- ×”×©×ª××© ×‘××¡×¤×¨×™× ×”××“×•×™×§×™× ×©××•×¤×™×¢×™× ×œ××¢×œ×”.
- ×× ×©×•××œ×™× ×¢×œ ×”×¢×•×‘×“ ×”××—×¨×•×Ÿ/×”× ×•×›×—×™ - ×§×— ××ª ×”×ª×œ×•×© ×”××—×¨×•×Ÿ.
"""

        # Create coordination task instance
        coordination_task_instance = Task(
            description=chatbot_coordination_task.description.format(
                user_question=request.message,
                context=context
            ),
            expected_output=chatbot_coordination_task.expected_output,
            agent=chatbot_coordination_task.agent
        )

        # Create crew with chatbot manager
        chatbot_crew = Crew(
            agents=[chatbot_manager, parser, validator, analyzer, designer],
            tasks=[coordination_task_instance],
            process=Process.sequential,
            verbose=True
        )

        # Execute
        print(f"[Chatbot] User asked: {request.message}")
        result = chatbot_crew.kickoff()
        response_text = str(result)

        # Save to chat history
        user_message = ChatHistory(
            session_id=request.session_id,
            role="user",
            message=request.message
        )
        assistant_message = ChatHistory(
            session_id=request.session_id,
            role="assistant",
            message=response_text,
            tools_used=["chatbot_manager"],
            extra_data={"agents_available": ["parser", "validator", "analyzer", "designer"]}
        )

        db.add(user_message)
        db.add(assistant_message)
        db.commit()

        # Save learning
        learning_manager.save_agent_execution(
            agent_name="chatbot_manager",
            task_type="coordinate",
            input_data={"question": request.message, "context": context[:500]},
            output_data=response_text[:1000],
            success=True,
            metadata={"session_id": request.session_id}
        )

        return {
            "success": True,
            "response": response_text,
            "session_id": request.session_id,
            "agents_used": ["chatbot_manager"]
        }

    except Exception as e:
        print(f"[Chatbot] Error: {e}")
        import traceback
        traceback.print_exc()

        # Save failed learning
        try:
            learning_manager.save_agent_execution(
                agent_name="chatbot_manager",
                task_type="coordinate",
                input_data={"question": request.message},
                output_data=str(e),
                success=False
            )
        except:
            pass

        raise HTTPException(status_code=500, detail=f"Chatbot error: {str(e)}")


class UpdateFileRequest(BaseModel):
    file_path: str
    content: str
    backup: bool = True


@app.post("/api/update-frontend")
async def update_frontend_file(request: UpdateFileRequest):
    """
    API endpoint for AI agents to update frontend files
    ×××¤×©×¨ ×œ×¡×•×›× ×™× (Designer/Chatbot) ×œ×¢×“×›×Ÿ ×§×‘×¦×™ frontend
    """
    try:
        # Validate file path - must be in frontend directory
        allowed_extensions = ['.html', '.css', '.js']
        if not any(request.file_path.endswith(ext) for ext in allowed_extensions):
            return {
                "success": False,
                "error": "×¨×§ ×§×‘×¦×™ HTML, CSS, ×•-JS ××•×ª×¨×™×"
            }

        # Build full path to frontend directory
        frontend_dir = Path("/app/frontend")  # In Docker container
        if not frontend_dir.exists():
            # Local development fallback
            frontend_dir = Path(__file__).parent.parent.parent / "frontend"

        target_file = frontend_dir / request.file_path

        # Security: prevent path traversal
        if not str(target_file.resolve()).startswith(str(frontend_dir.resolve())):
            return {
                "success": False,
                "error": "× ×ª×™×‘ ×§×•×‘×¥ ×œ× ×—×•×§×™"
            }

        # Backup existing file if requested
        if request.backup and target_file.exists():
            backup_path = target_file.with_suffix(target_file.suffix + '.backup')
            shutil.copy2(target_file, backup_path)
            print(f"âœ… Backup created: {backup_path}")

        # Write new content
        target_file.parent.mkdir(parents=True, exist_ok=True)
        target_file.write_text(request.content, encoding='utf-8')

        print(f"âœ… File updated by AI agent: {target_file}")

        return {
            "success": True,
            "message": f"×§×•×‘×¥ {request.file_path} ×¢×•×“×›×Ÿ ×‘×”×¦×œ×—×”!",
            "file_path": str(target_file)
        }

    except Exception as e:
        print(f"ğŸ”´ Error updating file: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


class VisualizationRequest(BaseModel):
    prompt: str
    month: str  # Format: "10/2025"


@app.post("/api/generate-visualization")
async def generate_visualization(
    request: VisualizationRequest,
    db: Session = Depends(get_db)
):
    """
    ×™×•×¦×¨ ×•×™×–×•××œ×™×–×¦×™×” ×‘×××¦×¢×•×ª Template Library
    ××©×ª××© ×‘-Claude ×¨×§ ×œ×–×™×”×•×™ ×¤×¨××˜×¨×™× (×–×•×œ!) ×•×œ× ×œ×‘× ×™×™×ª HTML
    """
    try:
        print(f"\nğŸ¨ Generating visualization...")
        print(f"   Prompt: {request.prompt}")
        print(f"   Month: {request.month}")

        # Parse month/year
        try:
            month_num, year_num = request.month.split('/')
            month_num = int(month_num)
            year_num = int(year_num)
        except:
            return {"success": False, "error": "×¤×•×¨××˜ ×—×•×“×© ×œ× ×ª×§×™×Ÿ"}

        # Use Claude to identify parameters (SHORT call = CHEAP!)
        import anthropic
        import json
        import re

        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

        identification_prompt = f"""
××ª×” ×¢×•×–×¨ ×©××–×”×” ×¤×¨××˜×¨×™× ××‘×§×©×•×ª ×œ××¤×¨×˜ visualization.

×‘×§×©×ª ×”××©×ª××©: "{request.prompt}"

×–×”×”:
1. ×¡×•×’: bar/line/pie/table/kpi
2. ××“×“:
   - final_payment (×©×›×¨ ×œ×ª×©×œ×•× / ×©×›×¨ ×¡×•×¤×™)
   - gross_salary (×©×›×¨ ×‘×¨×•×˜×• / ×¡×”"×› ×ª×©×œ×•××™×)
   - net_salary (×©×›×¨ × ×˜×•)
   - total_hours (×©×¢×•×ª ×¢×‘×•×“×”)
   - vacation_days (×™××™ ×—×•×¤×©×”)
   - sick_days (×™××™ ××—×œ×”)
3. ×§×™×‘×•×¥: employee/department (×× ×¨×œ×•×•× ×˜×™)
4. ×›×•×ª×¨×ª ××ª××™××”

×”×©×‘ ONLY ×‘-JSON:
{{
  "type": "bar",
  "metric": "final_payment",
  "group_by": "employee",
  "title": "×©×›×¨ ×œ×ª×©×œ×•× ×œ×¤×™ ×¢×•×‘×“×™×"
}}
"""

        message = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=200,
            messages=[
                {"role": "user", "content": identification_prompt}
            ]
        )

        response_text = message.content[0].text

        # Parse Claude response
        # Extract JSON from response
        json_match = re.search(r'\{[^{}]*\}', response_text)
        if not json_match:
            return {"success": False, "error": "×œ× ×”×¦×œ×—×ª×™ ×œ×”×‘×™×Ÿ ××ª ×”×‘×§×©×”"}

        params = json.loads(json_match.group())

        print(f"   Identified: {params}")

        # Build SQL query based on parameters
        metric_map = {
            "gross_salary": "gross_salary",
            "net_salary": "net_salary",
            "final_payment": "final_payment",
            "total_hours": "total_hours",
            "overtime_hours": "overtime_hours",
            "vacation_days": "vacation_days",
            "sick_days": "sick_days"
        }

        metric = params.get("metric", "final_payment")
        group_by = params.get("group_by", "employee")
        viz_type = params.get("type", "bar")
        title = params.get("title", "×ª×¦×•×’×”")

        # Query database
        if group_by == "employee":
            # Group by employee
            # Convert to strings since DB stores as VARCHAR
            payslips = db.query(Payslip).filter(
                Payslip.month == str(month_num),
                Payslip.year == str(year_num)
            ).all()

            # Extract data
            employee_data = {}
            for slip in payslips:
                emp_name = slip.employee_name or f"×¢×•×‘×“ {slip.employee_id}"

                # Get metric value from parsed_data
                value = 0
                if slip.parsed_data:
                    parsed = slip.parsed_data
                    if metric == "gross_salary":
                        # Gross salary is stored in additional_payments (sum of all payments)
                        additional = parsed.get("additional_payments", {})
                        value = sum([
                            float(additional.get("travel_allowance") or 0),
                            float(additional.get("tishrey_bonus") or 0),
                            float(additional.get("premium") or 0),
                            float(additional.get("base_wage") or 0),
                            float(additional.get("saturday_150") or 0),
                            float(additional.get("gift_value") or 0),
                            float(additional.get("severance_extra") or 0)
                        ])
                    elif metric == "final_payment":
                        # ×©×›×¨ ×œ×ª×©×œ×•× (final payment)
                        value = float(parsed.get("salary", {}).get("final_payment") or 0)
                    elif metric == "net_salary":
                        # ×©×›×¨ × ×˜×•
                        value = float(parsed.get("salary", {}).get("net") or 0)
                    elif metric == "total_hours":
                        value = float(parsed.get("work_hours") or 0)
                    elif metric == "vacation_days":
                        value = float(parsed.get("vacation_days") or 0)
                    elif metric == "sick_days":
                        value = float(parsed.get("sick_days") or 0)

                if emp_name in employee_data:
                    employee_data[emp_name] += value
                else:
                    employee_data[emp_name] = value

            labels = list(employee_data.keys())
            data = list(employee_data.values())

            print(f"   Found {len(payslips)} payslips")
            print(f"   Employee data: {employee_data}")
            print(f"   Labels: {labels}")
            print(f"   Data: {data}")

        else:
            # Default: just show all data
            labels = ["× ×ª×•× ×™×"]
            data = [100]

        # Build config for template
        if viz_type == "table":
            # For tables, we need columns and rows instead of labels/data
            columns = ["×©× ×¢×•×‘×“", params.get("title", "×¢×¨×š")]
            rows = [[label, f"{value:,.2f} â‚ª" if "salary" in metric else str(value)]
                    for label, value in zip(labels, data)]

            config = {
                "title": title,
                "columns": columns,
                "rows": rows
            }
        else:
            # For charts (bar, line, pie)
            config = {
                "title": title,
                "labels": labels,
                "data": data,
                "dataLabel": metric,
                "formatValue": "salary" in metric
            }

        print(f"   Final config: {config}")

        return {
            "success": True,
            "type": viz_type,
            "config": config
        }

    except Exception as e:
        print(f"ğŸ”´ Error generating visualization: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


# =====================
# Login Endpoint (NEW)
# =====================

from app.auth import authenticate, create_token

class LoginRequest(BaseModel):
    email: str
    password: str

@app.post("/api/login")
async def login(data: LoginRequest):
    """×”×ª×—×‘×¨×•×ª ×œ××¢×¨×›×ª"""
    if authenticate(data.email, data.password):
        token = create_token(data.email)
        return {
            "access_token": token,
            "token_type": "bearer",
            "user_email": data.email
        }
    raise HTTPException(status_code=401, detail="Email or password incorrect")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3000)
